{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMinTtvpVv/QpKgVVX6GAT9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krism-T/TRIAL-RF/blob/main/SKG_RANDOM_FOREST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3WXMItKtujhs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn streamlit pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUMyrYth55_T",
        "outputId": "238aa950-4646-4512-f484-4d7c657b2bb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.52.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.5.0 streamlit-1.52.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A. LOAD DATA\n",
        "# =========================\n",
        "sheet_url = 'https://docs.google.com/spreadsheets/d/1AfcS9SYlAba88BgWKhOIyKNR8TMlh379wvi8RlklVCI/edit?usp=sharing'\n",
        "csv_url = sheet_url.replace('/edit?usp=sharing', '/export?format=csv')\n",
        "\n",
        "df = pd.read_csv(csv_url)\n",
        "print(f\"Dataframe berhasil dimuat. Total baris: {len(df)}\")\n",
        "\n",
        "# Bersihkan NA\n",
        "df = df.dropna()\n",
        "print(f\"Total baris setelah drop NA: {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-ri-b3s6F8b",
        "outputId": "d89532ab-ac30-4013-c4f9-41b0664af7c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe berhasil dimuat. Total baris: 72\n",
            "Total baris setelah drop NA: 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B. PREPROCESSING\n",
        "# =========================\n",
        "TARGET_COLUMN = 'LevelKompetensi'\n",
        "\n",
        "# Semua variabel fitur sudah numerik â†’ tidak perlu OHE\n",
        "# Pilih semua kolom kecuali target & ID sebagai fitur\n",
        "feature_cols = [c for c in df.columns if c not in [TARGET_COLUMN, 'ID']]\n",
        "X = df[feature_cols].copy()\n",
        "\n",
        "# Pastikan benar-benar numerik (opsional, kalau sudah yakin boleh di-skip)\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "\n",
        "# Encode target\n",
        "le = LabelEncoder()\n",
        "df['y_encoded'] = le.fit_transform(df[TARGET_COLUMN])\n",
        "y = df['y_encoded']\n",
        "\n",
        "print(\"Mapping target (kode â†’ label):\")\n",
        "for code, label in enumerate(le.classes_):\n",
        "    print(code, \":\", label)\n",
        "\n",
        "# Tentukan kelas yang akan di-SMOTE: \"Novice Teacher\" dan \"Expert Teacher\"\n",
        "target_smote_labels = ['Novice Teacher', 'Expert Teacher']\n",
        "smote_classes = le.transform(target_smote_labels)  # ini dalam bentuk kode numerik\n",
        "print(\"\\nKelas yang akan di-oversample dengan SMOTE:\")\n",
        "for lbl, code in zip(target_smote_labels, smote_classes):\n",
        "    print(f\"{lbl} -> kode {code}\")\n",
        "\n",
        "# split train-test (tetap stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Ukuran X_train:\", X_train.shape)\n",
        "print(\"Ukuran X_test :\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umNvm2VM6Liw",
        "outputId": "26951b55-e66f-4974-e27e-a9ba94b1cfc8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping target (kode â†’ label):\n",
            "0 : Experienced Teacher\n",
            "1 : Expert Teacher\n",
            "2 : Novice Teacher\n",
            "\n",
            "Kelas yang akan di-oversample dengan SMOTE:\n",
            "Novice Teacher -> kode 2\n",
            "Expert Teacher -> kode 1\n",
            "Ukuran X_train: (57, 13)\n",
            "Ukuran X_test : (15, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ... (bagian load data & preprocessing kamu tetap sama)\n",
        "\n",
        "TARGET_COLUMN = 'LevelKompetensi'\n",
        "\n",
        "# fitur numerik (semua sudah numerik)\n",
        "feature_cols = [c for c in df.columns if c not in [TARGET_COLUMN, 'ID']]\n",
        "X = df[feature_cols].copy()\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "\n",
        "# encode target\n",
        "le = LabelEncoder()\n",
        "df['y_encoded'] = le.fit_transform(df[TARGET_COLUMN])\n",
        "y = df['y_encoded']\n",
        "\n",
        "print(\"Mapping target (kode â†’ label):\")\n",
        "for code, label in enumerate(le.classes_):\n",
        "    print(code, \":\", label)\n",
        "\n",
        "# ambil kode kelas untuk \"Novice Teacher\" dan \"Expert Teacher\"\n",
        "target_smote_labels = ['Novice Teacher', 'Expert Teacher']\n",
        "smote_classes = le.transform(target_smote_labels)\n",
        "print(\"\\nKelas yang akan di-oversample dengan SMOTE:\")\n",
        "for lbl, code in zip(target_smote_labels, smote_classes):\n",
        "    print(f\"{lbl} -> kode {code}\")\n",
        "\n",
        "# split train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Ukuran X_train:\", X_train.shape)\n",
        "print(\"Ukuran X_test :\", X_test.shape)\n",
        "\n",
        "# Determine global target count for minority classes based on majority class in full X_train\n",
        "train_class_counts = Counter(y_train)\n",
        "majority_class_code = train_class_counts.most_common(1)[0][0] # Get the code of the majority class\n",
        "target_count_for_minority = train_class_counts[majority_class_code] # Get its count\n",
        "\n",
        "# ============ FUNGSI STRATEGI SMOTE BARU ============== #\n",
        "def smote_strategy(y_array):\n",
        "    \"\"\"\n",
        "    y_array: array label di fold tertentu.\n",
        "    Mengembalikan dict: {kelas: target_jumlah_sampel}\n",
        "    Kelas minoritas ('Novice Teacher', 'Expert Teacher') akan di-oversample\n",
        "    hingga target_count_for_minority.\n",
        "    \"\"\"\n",
        "    counter = Counter(y_array)\n",
        "    strategy = {}\n",
        "\n",
        "    for cls_code in smote_classes: # Iterate through Novice and Expert Teacher codes\n",
        "        if cls_code in counter:\n",
        "            # Oversample to target_count_for_minority if current count is less\n",
        "            if counter[cls_code] < target_count_for_minority:\n",
        "                strategy[cls_code] = target_count_for_minority\n",
        "            else:\n",
        "                strategy[cls_code] = counter[cls_code] # If already at or above target, keep current count\n",
        "        # If class not in fold, it cannot be oversampled in this specific fold.\n",
        "        # So, we don't add it to the strategy dictionary.\n",
        "\n",
        "    # Ensure majority classes not explicitly targeted for oversampling are kept at their original count\n",
        "    for cls_code, count in counter.items():\n",
        "        if cls_code not in smote_classes:\n",
        "            strategy[cls_code] = count # Keep majority class count as is\n",
        "\n",
        "    return strategy\n",
        "\n",
        "# =========================\n",
        "# C. PIPELINE + GRIDSEARCH\n",
        "# =========================\n",
        "pipeline = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('smote', SMOTE(\n",
        "        random_state=42,\n",
        "        k_neighbors=1, # Reduced k_neighbors to be safer for small classes\n",
        "        sampling_strategy=smote_strategy\n",
        "    )),\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [100, 200],\n",
        "    'rf__max_depth': [10, 20, None],\n",
        "    'rf__min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nMemulai GridSearch...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "print(f\"\\nSkor F1-weighted terbaik (CV): {grid_search.best_score_:.4f}\")\n",
        "print(\"Best params:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_ERYFYR8eFY",
        "outputId": "bb98fa44-528d-45d1-a028-49c85ca7bfa6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping target (kode â†’ label):\n",
            "0 : Experienced Teacher\n",
            "1 : Expert Teacher\n",
            "2 : Novice Teacher\n",
            "\n",
            "Kelas yang akan di-oversample dengan SMOTE:\n",
            "Novice Teacher -> kode 2\n",
            "Expert Teacher -> kode 1\n",
            "Ukuran X_train: (57, 14)\n",
            "Ukuran X_test : (15, 14)\n",
            "\n",
            "Memulai GridSearch...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "\n",
            "Skor F1-weighted terbaik (CV): 0.9583\n",
            "Best params: {'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D. EVALUASI\n",
        "# =========================\n",
        "y_pred = best_pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n>>> AKURASI TEST: {accuracy:.4f}\\n\")\n",
        "print(\"--- Laporan Klasifikasi ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(pd.DataFrame(cm, index=le.classes_, columns=le.classes_))\n",
        "\n",
        "# =========================\n",
        "# E. SIMPAN MODEL + METADATA\n",
        "# =========================\n",
        "joblib.dump(best_pipeline, 'random_forest_pipeline.joblib')\n",
        "joblib.dump(le, 'label_encoder.joblib')\n",
        "joblib.dump(X.columns.tolist(), 'model_features.joblib')\n",
        "\n",
        "print(\"\\nâœ… Model, label encoder, dan daftar fitur berhasil disimpan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhzXC6Hz6bAb",
        "outputId": "d280d319-c1b1-4c3e-c5c0-5fb09d6c9f2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> AKURASI TEST: 0.9333\n",
            "\n",
            "--- Laporan Klasifikasi ---\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "Experienced Teacher       1.00      1.00      1.00        10\n",
            "     Expert Teacher       0.80      1.00      0.89         4\n",
            "     Novice Teacher       0.00      0.00      0.00         1\n",
            "\n",
            "           accuracy                           0.93        15\n",
            "          macro avg       0.60      0.67      0.63        15\n",
            "       weighted avg       0.88      0.93      0.90        15\n",
            "\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "                     Experienced Teacher  Expert Teacher  Novice Teacher\n",
            "Experienced Teacher                   10               0               0\n",
            "Expert Teacher                         0               4               0\n",
            "Novice Teacher                         0               1               0\n",
            "\n",
            "âœ… Model, label encoder, dan daftar fitur berhasil disimpan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_streamlit.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# ============================\n",
        "# 1. BACA MAPPING DARI EXCEL\n",
        "# ============================\n",
        "@st.cache_data\n",
        "def load_mapping_from_excel(path: str = \"MAPPING.xlsx\"):\n",
        "    df_map = pd.read_excel(path)\n",
        "\n",
        "    required_cols = {\"Variabel\", \"Code\", \"Meaning\"}\n",
        "    if not required_cols.issubset(set(df_map.columns)):\n",
        "        raise ValueError(f\"Kolom di MAPPING.xlsx harus mengandung: {required_cols}\")\n",
        "\n",
        "    mapping = {}\n",
        "    for var in df_map[\"Variabel\"].unique():\n",
        "        sub = df_map[df_map[\"Variabel\"] == var]\n",
        "        mapping[var] = {\n",
        "            str(row[\"Meaning\"]): int(row[\"Code\"])\n",
        "            for _, row in sub.iterrows()\n",
        "        }\n",
        "    return mapping\n",
        "\n",
        "try:\n",
        "    MAPPING = load_mapping_from_excel(\"MAPPING.xlsx\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Gagal memuat MAPPING.xlsx: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "categorical_features = list(MAPPING.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z499ZaMh8mxB",
        "outputId": "74a89b3d-9ffb-4879-a5be-8559663def60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_streamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. LOAD MODEL & METADATA\n",
        "# ============================\n",
        "try:\n",
        "    model_pipeline = joblib.load(\"random_forest_pipeline.joblib\")\n",
        "    le = joblib.load(\"label_encoder.joblib\")\n",
        "    model_features = joblib.load(\"model_features.joblib\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Gagal memuat model atau metadata (.joblib): {e}\")\n",
        "    st.stop()"
      ],
      "metadata": {
        "id": "xSElXrl48xvT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_streamlit.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from collections import Counter\n",
        "\n",
        "# ============================\n",
        "# 1. BACA MAPPING DARI EXCEL\n",
        "# ============================\n",
        "@st.cache_data\n",
        "def load_mapping_from_excel(path: str = \"MAPPING.xlsx\"):\n",
        "    df_map = pd.read_excel(path)\n",
        "\n",
        "    required_cols = {\"Variabel\", \"Code\", \"Meaning\"}\n",
        "    if not required_cols.issubset(set(df_map.columns)):\n",
        "        raise ValueError(f\"Kolom di MAPPING.xlsx harus mengandung: {required_cols}\")\n",
        "\n",
        "    mapping = {}\n",
        "    for var in df_map[\"Variabel\"].unique():\n",
        "        sub = df_map[df_map[\"Variabel\"] == var]\n",
        "        mapping[var] = {\n",
        "            str(row[\"Meaning\"]): int(row[\"Code\"])\n",
        "            for _, row in sub.iterrows()\n",
        "        }\n",
        "    return mapping\n",
        "\n",
        "try:\n",
        "    MAPPING = load_mapping_from_excel(\"MAPPING.xlsx\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Gagal memuat MAPPING.xlsx: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "categorical_features = list(MAPPING.keys())\n",
        "\n",
        "# ============================\n",
        "# 2. LOAD MODEL & METADATA\n",
        "# ============================\n",
        "try:\n",
        "    # Load label encoder first, as it's needed for smote_classes\n",
        "    le = joblib.load(\"label_encoder.joblib\")\n",
        "\n",
        "    # Define smote_classes and smote_strategy for model loading\n",
        "    target_smote_labels = ['Novice Teacher', 'Expert Teacher']\n",
        "    smote_classes = le.transform(target_smote_labels)\n",
        "\n",
        "    def smote_strategy(y_array):\n",
        "        counter = Counter(y_array)\n",
        "        max_count = max(counter.values())\n",
        "        strategy = {\n",
        "            cls: max_count\n",
        "            for cls in smote_classes\n",
        "            if cls in counter\n",
        "        }\n",
        "        return strategy\n",
        "\n",
        "    model_pipeline = joblib.load(\"random_forest_pipeline.joblib\")\n",
        "    model_features = joblib.load(\"model_features.joblib\")\n",
        "\n",
        "    # Load evaluation metrics\n",
        "    model_accuracy = joblib.load('model_accuracy.joblib')\n",
        "    model_classification_report = joblib.load('model_classification_report.joblib')\n",
        "    model_confusion_matrix = joblib.load('model_confusion_matrix.joblib')\n",
        "    model_roc_auc = joblib.load('model_roc_auc.joblib')\n",
        "\n",
        "except Exception as e:\n",
        "    st.error(f\"Gagal memuat model atau metadata (.joblib): {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# ============================\n",
        "# 3. UI STREAMLIT\n",
        "# ============================\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title(\"ðŸ‘¨â€ðŸ« Prediksi Level Kompetensi Guru (SKG 360)\")\n",
        "\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "Aplikasi ini menggunakan model **Random Forest** untuk memprediksi level kompetensi guru berdasarkan data demografis/latar belakang guru.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Initialize session state for form key if not already present\n",
        "if 'form_key_counter' not in st.session_state:\n",
        "    st.session_state.form_key_counter = 0\n",
        "\n",
        "# Function to increment the form key, effectively resetting the form\n",
        "def reset_form():\n",
        "    st.session_state.form_key_counter += 1\n",
        "\n",
        "# Place the 'New Input' button outside the form for a full reset\n",
        "# Add 'on_click' to trigger the reset_form function\n",
        "st.button(\"Input Baru\", on_click=reset_form)\n",
        "\n",
        "def _options(var_name):\n",
        "    opts = list(MAPPING.get(var_name, {}).keys())\n",
        "    # Add a placeholder for an empty selection\n",
        "    return [\"--Pilih--\"] + opts if len(opts) > 0 else [\"--Pilih--\", \"(tidak ada data)\"]\n",
        "\n",
        "with st.form(key=f\"my_form_{st.session_state.form_key_counter}\"):\n",
        "    st.subheader(\"Input Data Guru\")\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        # Remove default value, will default to min_value\n",
        "        usia = st.number_input(\"Usia (tahun)\", min_value=20, max_value=70)\n",
        "        input_strings = {}\n",
        "        input_strings[\"Sekolah\"] = st.selectbox(\"Sekolah\", _options(\"Sekolah\"))\n",
        "        input_strings[\"Departemen\"] = st.selectbox(\"Departemen\", _options(\"Departemen\"))\n",
        "        input_strings[\"Gender\"] = st.selectbox(\"Gender\", _options(\"Gender\"))\n",
        "        input_strings[\"Mapel\"] = st.selectbox(\"Mata Pelajaran\", _options(\"Mapel\"))\n",
        "\n",
        "    with col2:\n",
        "        # Remove default value, will default to min_value\n",
        "        pengalaman = st.number_input(\"Pengalaman Mengajar (tahun)\", min_value=0, max_value=40)\n",
        "\n",
        "        input_strings[\"Region Lahir\"] = st.selectbox(\"Region Lahir\", _options(\"Region Lahir\"))\n",
        "        input_strings[\"katalog.StatusKaryawanChoices\"] = st.selectbox(\n",
        "            \"Status Karyawan\", _options(\"katalog.StatusKaryawanChoices\")\n",
        "        )\n",
        "        input_strings[\"katalog.MaritalStatus\"] = st.selectbox(\n",
        "            \"Status Perkawinan\", _options(\"katalog.MaritalStatus\")\n",
        "        )\n",
        "        input_strings[\"TOEFL>400\"] = st.selectbox(\n",
        "            \"Memiliki Sertifikat TOEFL â‰¥ 400?\", _options(\"TOEFL>400\")\n",
        "        )\n",
        "        input_strings[\"Pendidikan\"] = st.selectbox(\n",
        "            \"Pendidikan Terakhir\", _options(\"Pendidikan\")\n",
        "        )\n",
        "        input_strings[\"Prodi\"] = st.selectbox(\"Program Studi S1\", _options(\"Prodi\"))\n",
        "        input_strings[\"UnivS1\"] = st.selectbox(\"Universitas S1\", _options(\"UnivS1\"))\n",
        "\n",
        "    submitted = st.form_submit_button(\"Prediksi Level Kompetensi\")\n",
        "\n",
        "if submitted:\n",
        "    # Initialize a flag to check if we can proceed with prediction\n",
        "    can_predict = True\n",
        "\n",
        "    # ============================\n",
        "    # 4. BANGUN DATAFRAME INPUT\n",
        "    # ============================\n",
        "    raw_data = {\n",
        "        \"katalog.Usia\": usia,\n",
        "        \"Pengalaman\": pengalaman\n",
        "    }\n",
        "\n",
        "    for kol, label_str in input_strings.items():\n",
        "        # Validate that a selection has been made for selectbox inputs\n",
        "        if label_str == \"--Pilih--\":\n",
        "            st.error(f\"Mohon pilih nilai untuk '{kol}'.\")\n",
        "            can_predict = False\n",
        "            continue # Continue to check other inputs\n",
        "\n",
        "        try:\n",
        "            raw_data[kol] = MAPPING[kol][label_str]\n",
        "        except KeyError:\n",
        "            st.error(f\"Nilai '{label_str}' tidak ditemukan di MAPPING untuk variabel '{kol}'.\")\n",
        "            can_predict = False\n",
        "            continue # Continue to check other inputs\n",
        "\n",
        "    # ============================\n",
        "    # 5. OHE & ALIGN DENGAN MODEL\n",
        "    # ============================\n",
        "    if can_predict: # Only proceed if all validations passed\n",
        "        input_df = pd.DataFrame([raw_data])\n",
        "        input_encoded = pd.get_dummies(input_df, columns=categorical_features, drop_first=True)\n",
        "\n",
        "        final_input = pd.DataFrame(columns=model_features).fillna(0)\n",
        "        for col in input_encoded.columns:\n",
        "            if col in final_input.columns:\n",
        "                final_input[col] = input_encoded[col]\n",
        "\n",
        "        final_input = final_input.fillna(0)\n",
        "        final_input = final_input[model_features]\n",
        "\n",
        "        # ============================\n",
        "        # 6. PREDIKSI\n",
        "        # ============================\n",
        "        try:\n",
        "            pred_encoded = model_pipeline.predict(final_input)\n",
        "            pred_label = le.inverse_transform(pred_encoded)[0]\n",
        "\n",
        "            proba = None\n",
        "            try:\n",
        "                proba = model_pipeline.predict_proba(final_input)[0]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            st.success(f\"### Hasil Prediksi Level Kompetensi: **{pred_label}**\")\n",
        "\n",
        "            if proba is not None:\n",
        "                st.markdown(\"**Probabilitas tiap level:**\")\n",
        "                prob_df = pd.DataFrame({\n",
        "                    \"LevelKompetensi\": le.classes_,\n",
        "                    \"Probabilitas\": proba\n",
        "                }).sort_values(\"Probabilitas\", ascending=False)\n",
        "                st.dataframe(prob_df, use_container_width=True)\n",
        "\n",
        "            if \"Expert\" in pred_label:\n",
        "                st.balloons()\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Terjadi error saat prediksi: {e}\")\n",
        "\n",
        "# ============================\n",
        "# 7. TAMPILKAN METRIK EVALUASI\n",
        "# ============================\n",
        "st.markdown(\"--- \")\n",
        "st.subheader(\"Evaluasi Performa Model (pada Test Set)\")\n",
        "\n",
        "expander_metrics = st.expander(\"Lihat Metrik Evaluasi Model\")\n",
        "with expander_metrics:\n",
        "    st.write(f\"**Akurasi**: {model_accuracy:.4f}\")\n",
        "    st.write(f\"**ROC-AUC (weighted)**: {model_roc_auc:.4f}\")\n",
        "    st.markdown(\"**Classification Report:**\")\n",
        "    st.code(model_classification_report, language='text')\n",
        "    st.markdown(\"**Confusion Matrix:**\")\n",
        "    st.dataframe(model_confusion_matrix, use_container_width=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xT1vWgR8zlC",
        "outputId": "35b485c6-feaf-4ed9-ef34-9e2e3d93cdc8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_streamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess, time\n",
        "import os\n",
        "\n",
        "# isi tokenmu di sini\n",
        "ngrok.set_auth_token(\"36iFdHX5BlJ6rMGCg5TlRKDZY5B_3qaXrb3oqxKnwnakdPgfp\")  # ganti ya\n",
        "\n",
        "current_streamlit_proc = None\n",
        "\n",
        "def stop_streamlit():\n",
        "    \"\"\"Matikan Streamlit & ngrok (jika ada).\"\"\"\n",
        "    global current_streamlit_proc\n",
        "    if current_streamlit_proc is not None:\n",
        "        try:\n",
        "            current_streamlit_proc.terminate()\n",
        "            current_streamlit_proc.wait(timeout=5)\n",
        "        except:\n",
        "            try:\n",
        "                current_streamlit_proc.kill()\n",
        "            except:\n",
        "                pass\n",
        "        current_streamlit_proc = None\n",
        "\n",
        "    try:\n",
        "        ngrok.kill()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    os.system(\"fuser -k 8501/tcp >/dev/null 2>&1\")\n",
        "    print(\"âœ… Streamlit & ngrok (jika ada) sudah dimatikan.\")\n",
        "\n",
        "def start_streamlit():\n",
        "    \"\"\"Stop instance lama, lalu jalankan Streamlit + ngrok baru.\"\"\"\n",
        "    global current_streamlit_proc\n",
        "\n",
        "    stop_streamlit()\n",
        "\n",
        "    print(\"ðŸš€ Menjalankan Streamlit...\")\n",
        "    proc = subprocess.Popen(\n",
        "        [\"streamlit\", \"run\", \"app_streamlit.py\",\n",
        "         \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "    current_streamlit_proc = proc\n",
        "\n",
        "    time.sleep(10)\n",
        "\n",
        "    print(\"ðŸŒ Membuat tunnel ngrok...\")\n",
        "    tunnel = ngrok.connect(8501, bind_tls=True)\n",
        "\n",
        "    print(\"âœ… Aplikasi Streamlit siap diakses di URL:\")\n",
        "    print(tunnel.public_url)\n",
        "    return tunnel.public_url\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQIolyzc89Tp",
        "outputId": "87905e00-bb3f-4840-b286-5293bf5346f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35c66a52",
        "outputId": "a25b1a5b-5a62-4bb0-8ff7-088c8e14a23c"
      },
      "source": [
        "##RE-READ FILE MAPPING\n",
        "import requests\n",
        "\n",
        "sheet_url = 'https://docs.google.com/spreadsheets/d/1K74HlgKj19djz9EUeg0ZUIWMzM9Kt4TvS0oPn_CDsSc/edit?usp=sharing'\n",
        "excel_export_url = sheet_url.replace('/edit?usp=sharing', '/export?format=xlsx')\n",
        "\n",
        "file_name = 'MAPPING.xlsx'\n",
        "\n",
        "try:\n",
        "    response = requests.get(excel_export_url)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "    with open(file_name, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"File '{file_name}' berhasil diunduh dan disimpan.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Gagal mengunduh file: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Terjadi kesalahan: {e}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'MAPPING.xlsx' berhasil diunduh dan disimpan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "af573fc0",
        "outputId": "02d9addf-bf64-4170-c1ed-c1680fc62d34"
      },
      "source": [
        "url = start_streamlit()\n",
        "url"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Streamlit & ngrok (jika ada) sudah dimatikan.\n",
            "ðŸš€ Menjalankan Streamlit...\n",
            "ðŸŒ Membuat tunnel ngrok...\n",
            "âœ… Aplikasi Streamlit siap diakses di URL:\n",
            "https://zona-squirelike-diedra.ngrok-free.dev\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://zona-squirelike-diedra.ngrok-free.dev'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe9aff0",
        "outputId": "fb4bdae1-2a3f-4787-c759-44115193dbdc"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_test and best_pipeline are available from previous cells\n",
        "# For multi-class, we need to handle ROC-AUC differently, e.g., 'ovr' or 'ovo'\n",
        "# We'll use 'ovr' (One-vs-Rest) strategy and average='weighted'\n",
        "\n",
        "y_pred_proba = best_pipeline.predict_proba(X_test)\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "# For multi-class, we calculate the ROC AUC for each class and then average them.\n",
        "# We'll use 'ovr' (One-vs-Rest) strategy and average='weighted'\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "\n",
        "print(f\"\\n>>> ROC-AUC Score (weighted, One-vs-Rest): {roc_auc:.4f}\")\n",
        "\n",
        "print(f\"\\n>>> AKURASI TEST: {accuracy:.4f}\\n\")\n",
        "print(\"--- Laporan Klasifikasi ---\")\n",
        "classification_rep_str = classification_report(y_test, y_pred, target_names=le.classes_)\n",
        "print(classification_rep_str)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(cm_df)\n",
        "\n",
        "# Save evaluation metrics\n",
        "joblib.dump(accuracy, 'model_accuracy.joblib')\n",
        "joblib.dump(classification_rep_str, 'model_classification_report.joblib')\n",
        "joblib.dump(cm_df, 'model_confusion_matrix.joblib')\n",
        "joblib.dump(roc_auc, 'model_roc_auc.joblib')\n",
        "\n",
        "print(\"\\nâœ… Metrik evaluasi model berhasil disimpan.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> ROC-AUC Score (weighted, One-vs-Rest): 1.0000\n",
            "\n",
            ">>> AKURASI TEST: 0.9333\n",
            "\n",
            "--- Laporan Klasifikasi ---\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "Experienced Teacher       1.00      1.00      1.00        10\n",
            "     Expert Teacher       0.80      1.00      0.89         4\n",
            "     Novice Teacher       0.00      0.00      0.00         1\n",
            "\n",
            "           accuracy                           0.93        15\n",
            "          macro avg       0.60      0.67      0.63        15\n",
            "       weighted avg       0.88      0.93      0.90        15\n",
            "\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "                     Experienced Teacher  Expert Teacher  Novice Teacher\n",
            "Experienced Teacher                   10               0               0\n",
            "Expert Teacher                         0               4               0\n",
            "Novice Teacher                         0               1               0\n",
            "\n",
            "âœ… Metrik evaluasi model berhasil disimpan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpX3rTH49ZvE"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}